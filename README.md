# eo-pipelines

Define and run pipelines to process Earth Observation (EO) data.  

This project main focus has been on processing Landsat imagery.

## Installation

create a conda environment called `eo_pipelines_env` using:

```
conda env create -f eo_pipelines_env.yml
```

## Dependencies

Requires the hyrrokkin, usgs, landsat_importer and pyjob libraries

- https://github.com/surftemp/pyjob
- https://github.com/surftemp/usgs
- https://github.com/surftemp/landsat_importer
- https://github.com/visualtopology/hyrrokkin

## Defining Pipelines

Pipelines are defined using a YAML file, for example `test_pipeline.yaml`, that starts with a configuration section:

```yaml
configuration:
  eo_pipelines:
    # the spec describes global parameters
    spec:
      # define the region of interest
      lat_min: "72.35.56N"
      lat_max: "64.44.52N"
      lon_min: "54.46.05W"
      lon_max: "53.49.56W"
      # and the time range
      start_date: 2021-06-01
      end_date: 2021-08-31
      # filter out scenes thought to have more than 10% cloud cover
      max_cloud_cover_fraction: 0.1
      # specify the datasets of interest
      datasets:
        LANDSAT_OT_C2_L1:
          bands: [ 1,2,3,4,5,6,7,8,9,10,11,QA_PIXEL ]
    # the environment describes how 
    environment:
      working_directory: /tmp/pipeline_test
      conda_path: /home/users/niallmcc/miniconda3/bin/conda
      shell: "/bin/bash"
      echo_stdout: True
      tracking_database_path: /tmp/tracking.db # path to a database that is used to record task execution statistics
```

Following the configuration, each stage in the pipeline is described

```yaml
nodes:
  n0:
    # searches the region and time of interest for matching scenes
    type: eo_pipelines:usgs_search
  n1:
    # fetch the scenes from the USGS 
    type: eo_pipelines:usgs_fetch
    properties:
      configuration:
        output_path: fetched
  n2:
    # decode and import the scenes to individual netcdf4 files
    type: eo_pipelines:landsat_import
    properties:
      configuration:
        output_path: imported
      executor_settings:
          executor: local
          local:
            nr_threads: 4
```

Finally, the links between the nodes are specified to complete the pipeline's definition

```yaml
links:
  - n0 => n1
  - n1 => n2
  - n2 => n3
```

On execution the directory structure will be created:

```
/tmp/
    pipeline_test/
        n0/
            LANDSAT_OT_C2_L1_scenes.csv
        n1/
            fetched/
                LANDSAT_OT_C2_L1/
                    LC08_L1TP_005003_20140605_20200911_02_T1_B2.TIF   
                    LC08_L1TP_005003_20160610_20200906_02_T1_B4.TIF   
                    LC08_L1TP_005003_20190603_20200828_02_T1_B6.TIF   
                    ...
        n2/
            imported/
                LANDSAT_OT_C2_L1/
                    20140605143221-NCEO-L1C-Landsat8-v2.0-fv01.0.nc
                    20150608143155-NCEO-L1C-Landsat8-v2.0-fv01.0.nc 
                    ...
```

## Configurations and Executor settings

Each node is defined by a `properties` section which is divided into `configuration` and `executor_settings`

* `properties` -> `configutation`

In this section, parameters that are passed to the node are defined.  In the example above, nodes `n1` and `n2` are both configured with an `output_path`
parameter which controls where the node's output files are written.

* `properties` -> `executor_settings`

Most nodes execute themselves by launching one or more tasks, which can be run on the local machine, or as SLURM jobs.  The `executor_settings` configuration determines how tasks are executed

executor_setting | purpose
------------ | -------------------------------
**executor** | set to either "local" or "slurm"
**can_skip** | set to "true" or "false" (the default).  Allows the executor to skip execution of the node if a results.json file has been generated by a previous execution.
**concurrent_execution_limit -> limit** | limit the number of executions that can happen, systemwide
**concurrent_execution_limit -> tracking_folder** | the shared folder used to implement the concurrent execution limit
**retry_count** | the number of times a failed task should be retried
**local -> nr_threads** | the number of threads/sub-processes to launch in parallel, when executing tasks locally


## Available stages

Pipleline stages currently supported:

stage_name | purpose                                                                            | required conda environment
-----------|------------------------------------------------------------------------------------| --------------------------
**usgs_search** | find landsat scenes using the usgs tool                                            | usgs_env
**usgs_fetch** | download landsat scenes using the usgs_download tool                               | usgs_env
**landsat_import** | convert landsat scenes to netcdf4                                                  | rioxarray_env
**usgs_group** | combine/stack landsat scenes from the same orbit (also across different datasets)  | pyproj_env
**xesmf_regrid** | regrid scenes onto a target grid using XESMF                                       | xesmf_env
**add_masks** | add mask layers defined by geojson files                                           | cartopy_env
**landsat_align** | perform the intersection of a set of landsat scenes from the same row/path         | xesmf_env
**netcdf2html** | create an interactive static HTML website for exploring netcdf files               | cartopy_env
**time_stacker** | combine a number of separate netcdf4 files into one file by appending along the time access | pyproj_env

## Required conda environments

Each of the above stages requires a conda environment and specific tools installed into it.  For details on how to create these see:

### usgs_env (usgs_search, usgs_fetch)

See https://github.com/surftemp/usgs

Environment variables USGS_USERNAME and USGS_PASSWORD should be defined

### rioxarray_env (landsat_import)

See https://github.com/surftemp/landsat_importer

### cartopy_env (netcdf2html add_masks)

See https://github.com/surftemp/eo-pipelines/tree/main/environments/cartopy

### pyproj_env (usgs_group, time_stacker)

https://github.com/surftemp/eo-pipelines/tree/main/environments/pyproj_env

### xesmf_env (xesmf_regrid)

https://github.com/surftemp/eo-pipelines/tree/main/environments/xesmf

## Running pipelines

See the `example_pipelines` folder for examples of how to define pipelines

To run a single pipeline:

```
conda activate eo_pipelines_env
nohup run_pipeline <path-to-pipeline-yaml-file> &
```

## Running mulitple pipelines

To run a set of pipelines (pipelines are executed in series):

```
nohup run_pipelines "**/*.yaml" &
```

A tool is provided for preparing a set of pipelines from a template and a CSV file containing parameters that will be substituted into the template

```
python -m eo_pipelines.utils.template_to_pipelines.py pipeline_template.yaml parameters.csv /tmp/pipelines
```

where pipeline_template.yaml is a pipeline with template parameters surrounded by curly brackets

```
nodes:
...
  n0:
    type: eo_pipelines:usgs_search
    properties:
      executor_settings:
        can_skip: true
      configuration:
        row: {row}
        path: {path}
        months: [6]
...
```

and parameters.csv contains values to substitute:

```csv
row,path
23,202
22,207
26,201
18,209
```

running the tool will create the following directory structure

```filesystem
/tmp/
    pipelines/
        pipeline0/
            pipeline.yaml
        pipeline1/
            pipeline.yaml
```
